{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.utils.model import TTN_wrapper\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of pytorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3760, 0.7771, 0.4800],\n",
      "        [0.1252, 0.1105, 0.0270],\n",
      "        [0.4306, 0.4752, 0.1102],\n",
      "        [0.7243, 0.3499, 0.7969],\n",
      "        [0.1168, 0.6603, 0.4248]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "69\n",
      "2.1.3\n",
      "{'fit_loop': {'state_dict': {}, 'epoch_loop.state_dict': {'_batches_that_stepped': 25059}, 'epoch_loop.batch_progress': {'total': {'ready': 25060, 'completed': 25060, 'started': 25060, 'processed': 25060}, 'current': {'ready': 358, 'completed': 358, 'started': 358, 'processed': 358}, 'is_last_batch': True}, 'epoch_loop.scheduler_progress': {'total': {'ready': 0, 'completed': 0}, 'current': {'ready': 0, 'completed': 0}}, 'epoch_loop.automatic_optimization.state_dict': {}, 'epoch_loop.automatic_optimization.optim_progress': {'optimizer': {'step': {'total': {'ready': 0, 'completed': 0}, 'current': {'ready': 0, 'completed': 0}}, 'zero_grad': {'total': {'ready': 0, 'completed': 0, 'started': 0}, 'current': {'ready': 0, 'completed': 0, 'started': 0}}}}, 'epoch_loop.manual_optimization.state_dict': {}, 'epoch_loop.manual_optimization.optim_step_progress': {'total': {'ready': 75180, 'completed': 75180}, 'current': {'ready': 75180, 'completed': 75180}}, 'epoch_loop.val_loop.state_dict': {}, 'epoch_loop.val_loop.batch_progress': {'total': {'ready': 90, 'completed': 90, 'started': 90, 'processed': 90}, 'current': {'ready': 90, 'completed': 90, 'started': 90, 'processed': 90}, 'is_last_batch': True}, 'epoch_progress': {'total': {'ready': 70, 'completed': 69, 'started': 70, 'processed': 69}, 'current': {'ready': 70, 'completed': 69, 'started': 70, 'processed': 69}}}, 'validate_loop': {'state_dict': {}, 'batch_progress': {'total': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'is_last_batch': False}}, 'test_loop': {'state_dict': {}, 'batch_progress': {'total': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'is_last_batch': False}}, 'predict_loop': {'state_dict': {}, 'batch_progress': {'total': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}}}}\n",
      "{\"ModelCheckpoint{'monitor': 'eer', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\": {'monitor': 'eer', 'best_model_score': tensor(0.1973), 'best_model_path': '/home/fbi1532/Projects/tattoo-retrieval/runs_dual/efficientnet_v2_s_4.0_0.5_256/epoch=69-eer=0.20.ckpt', 'current_score': tensor(0.1973), 'dirpath': '/home/fbi1532/Projects/tattoo-retrieval/runs_dual/efficientnet_v2_s_4.0_0.5_256', 'best_k_models': {'/home/fbi1532/Projects/tattoo-retrieval/runs_dual/efficientnet_v2_s_4.0_0.5_256/epoch=69-eer=0.20.ckpt': tensor(0.1973)}, 'kth_best_model_path': '/home/fbi1532/Projects/tattoo-retrieval/runs_dual/efficientnet_v2_s_4.0_0.5_256/epoch=69-eer=0.20.ckpt', 'kth_value': tensor(0.1973), 'last_model_path': ''}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.abspath('data/tattoos/tatt_trn/pretrained_models/tatt_trn_pretrained_model.ckpt'), map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TTN_wrapper(model='efficientnet_v2_s', num_features=256, num_identities=457)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 288, 32])\n",
      "(tensor([[[[0.9377, 0.9589, 0.9327,  ..., 0.9554, 0.9158, 0.6871],\n",
      "          [0.9484, 0.9945, 0.9980,  ..., 0.9877, 0.9950, 0.9695],\n",
      "          [0.9693, 0.9961, 0.9942,  ..., 0.9951, 0.9968, 0.9766],\n",
      "          ...,\n",
      "          [0.7832, 0.9380, 0.9575,  ..., 0.9999, 1.0000, 0.9993],\n",
      "          [0.9200, 0.9709, 0.9617,  ..., 0.9994, 0.9997, 0.9938],\n",
      "          [0.8283, 0.8955, 0.8324,  ..., 0.9877, 0.9919, 0.9179]],\n",
      "\n",
      "         [[0.9669, 0.9935, 0.9889,  ..., 0.9960, 0.9984, 0.9595],\n",
      "          [0.9736, 0.9919, 0.9892,  ..., 0.9912, 0.9970, 0.9491],\n",
      "          [0.9654, 0.9938, 0.9966,  ..., 0.9784, 0.9975, 0.9265],\n",
      "          ...,\n",
      "          [0.8805, 0.9850, 0.9945,  ..., 0.9993, 0.9997, 0.9331],\n",
      "          [0.7904, 0.9702, 0.9897,  ..., 0.9997, 0.9998, 0.9913],\n",
      "          [0.5743, 0.8196, 0.9150,  ..., 0.9838, 0.9871, 0.8785]],\n",
      "\n",
      "         [[0.9682, 0.9952, 0.9944,  ..., 0.9982, 0.9991, 0.9823],\n",
      "          [0.9931, 0.9988, 0.9965,  ..., 0.9939, 0.9986, 0.9866],\n",
      "          [0.9858, 0.9984, 0.9861,  ..., 0.9815, 0.9968, 0.9880],\n",
      "          ...,\n",
      "          [0.9569, 0.9949, 0.9925,  ..., 0.9996, 0.9999, 0.9968],\n",
      "          [0.9250, 0.9963, 0.9973,  ..., 0.9980, 0.9995, 0.9708],\n",
      "          [0.7403, 0.9774, 0.9655,  ..., 0.9724, 0.9828, 0.8835]]]],\n",
      "       grad_fn=<TanhBackward0>), tensor([[[[ 0.9323,  0.9991,  0.9999,  ...,  0.9570,  0.9542,  0.8352],\n",
      "          [ 0.9734,  1.0000,  1.0000,  ...,  0.9985,  0.9973,  0.9606],\n",
      "          [ 0.9957,  1.0000,  1.0000,  ...,  0.9990,  0.9964,  0.9846],\n",
      "          ...,\n",
      "          [-0.5074, -0.6073, -0.5095,  ..., -0.6636, -0.4979, -0.2482],\n",
      "          [-0.6952, -0.5918, -0.6024,  ..., -0.4705, -0.0361,  0.1366],\n",
      "          [-0.7486, -0.7841, -0.6908,  ..., -0.2674, -0.0636, -0.1009]],\n",
      "\n",
      "         [[ 0.9882,  0.9771,  0.9850,  ...,  0.4962,  0.0094,  0.3675],\n",
      "          [ 0.9998,  0.9999,  0.9997,  ...,  0.9623,  0.6371,  0.6147],\n",
      "          [ 0.9999,  0.9999,  1.0000,  ...,  0.9940,  0.9596,  0.8592],\n",
      "          ...,\n",
      "          [-0.8921, -0.9920, -0.9961,  ..., -0.9881, -0.9851, -0.9140],\n",
      "          [-0.9148, -0.9868, -0.9940,  ..., -0.9829, -0.9344, -0.9011],\n",
      "          [-0.5807, -0.8308, -0.9179,  ..., -0.8362, -0.7415, -0.6202]],\n",
      "\n",
      "         [[ 0.5538,  0.5707,  0.9089,  ...,  0.4588,  0.5269,  0.5788],\n",
      "          [ 0.9496,  0.9956,  0.9960,  ...,  0.5397,  0.3604, -0.2046],\n",
      "          [ 0.9749,  0.9981,  0.9989,  ...,  0.6661,  0.6004,  0.2940],\n",
      "          ...,\n",
      "          [-0.9698, -0.9819, -0.9855,  ..., -0.9784, -0.9502, -0.5841],\n",
      "          [-0.8763, -0.9454, -0.8925,  ..., -0.9239, -0.9181, -0.3084],\n",
      "          [-0.8546, -0.9339, -0.9162,  ..., -0.8773, -0.8298, -0.4788]]]],\n",
      "       grad_fn=<TanhBackward0>), tensor([[-0.2687, -0.3536, -0.4500, -0.0317,  0.1804, -0.0080, -0.1901,  0.1415,\n",
      "         -0.0416, -0.0752, -0.3151,  0.0138, -0.4318, -0.3127, -0.4108,  0.2212,\n",
      "         -0.3311,  0.0240, -0.1218,  0.0246, -0.2151, -0.0640, -0.1835,  0.4265,\n",
      "         -0.1249,  0.3483,  0.1741, -0.3095, -0.4997, -0.1680,  0.2088,  0.3214,\n",
      "         -0.3801,  0.4602, -0.1447, -0.1346,  0.1175, -0.1821, -0.3425, -0.1643,\n",
      "          0.0719,  0.2085, -0.2246,  0.0152, -0.3200,  0.0470,  0.0440,  0.2071,\n",
      "          0.1712, -0.1230,  0.0512,  0.2804,  0.0673, -0.1871,  0.0348,  0.1597,\n",
      "         -0.1173,  0.4583,  0.0946, -0.2045, -0.2937,  0.0638, -0.0283,  0.2915,\n",
      "         -0.3452, -0.2475,  0.2115, -0.3355, -0.4112,  0.2974, -0.2836,  0.1678,\n",
      "         -0.0162,  0.1614,  0.3686, -0.3498, -0.0632, -0.0198, -0.3464, -0.1766,\n",
      "         -0.4271,  0.0993,  0.0745,  0.2911,  0.3030, -0.1384,  0.2178, -0.4478,\n",
      "          0.0156, -0.0477,  0.2004, -0.0220, -0.0109, -0.3856,  0.2340,  0.0625,\n",
      "         -0.0925, -0.2646,  0.1066, -0.0386,  0.3905,  0.0158, -0.0534, -0.1104,\n",
      "         -0.0959, -0.0765, -0.1619,  0.1973, -0.0303,  0.0659,  0.0249, -0.2098,\n",
      "         -0.3789, -0.1905,  0.1969,  0.2017,  0.3931, -0.6571,  0.3171,  0.2489,\n",
      "         -0.2733, -0.2992,  0.2489, -0.4156,  0.1026, -0.3875, -0.2331, -0.0707,\n",
      "         -0.2340, -0.3214, -0.4173,  0.0209, -0.4544,  0.0148, -0.3051,  0.3959,\n",
      "         -0.3183, -0.0454,  0.0370, -0.0127,  0.1248, -0.4065, -0.2319, -0.0585,\n",
      "          0.2795, -0.4015, -0.1863,  0.1977,  0.1440,  0.2867,  0.3673,  0.1110,\n",
      "          0.2278, -0.2831,  0.2964, -0.1468,  0.0073, -0.0650,  0.0130,  0.2814,\n",
      "          0.0315,  0.2000,  0.1550, -0.1573,  0.1425,  0.3844,  0.0540, -0.3189,\n",
      "         -0.4270, -0.1305, -0.1817, -0.3106, -0.3910,  0.1141,  0.4780,  0.1611,\n",
      "         -0.1782,  0.2986, -0.1887,  0.1417,  0.1396, -0.0732, -0.2623,  0.4412,\n",
      "         -0.1445, -0.1095, -0.0741,  0.1595,  0.1852,  0.0824,  0.1330,  0.0258,\n",
      "         -0.2209,  0.2266,  0.1203,  0.1088,  0.0496, -0.1813, -0.0362, -0.2301,\n",
      "         -0.0170,  0.2840,  0.1786, -0.4188, -0.0885,  0.2281, -0.2886,  0.2227,\n",
      "         -0.1490,  0.0695,  0.2695, -0.4449,  0.1815, -0.2497,  0.0477,  0.1832,\n",
      "          0.0296,  0.3254,  0.1078,  0.1722, -0.4246,  0.0008, -0.1365,  0.0609,\n",
      "          0.1617, -0.5447, -0.3656,  0.2427, -0.3197,  0.1140, -0.1241,  0.0990,\n",
      "         -0.0979,  0.0015, -0.0271,  0.2200,  0.0652,  0.1469, -0.1666,  0.0305,\n",
      "          0.0297, -0.0817, -0.0240,  0.2082, -0.1364, -0.0184,  0.3774,  0.1657,\n",
      "         -0.1272, -0.1487,  0.1154,  0.3547,  0.0507,  0.3387, -0.0481, -0.0703]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.2440e-02, -1.7503e-01,  1.8192e-02,  5.5715e-02, -1.8140e-01,\n",
      "         -4.3077e-02,  1.8977e-01,  2.5379e-01, -2.3873e-01,  1.9191e-01,\n",
      "         -1.3021e-01,  1.3402e-01,  9.6763e-02,  1.2825e-01, -6.6339e-02,\n",
      "         -7.5121e-02, -2.1120e-01,  1.1004e-01,  1.5941e-01, -4.1124e-02,\n",
      "         -3.4180e-02,  1.3255e-01, -1.3523e-01, -8.1963e-02,  6.8982e-02,\n",
      "          5.1586e-02,  1.7706e-01,  1.2949e-01, -3.1997e-02,  7.5594e-02,\n",
      "         -1.2596e-02,  7.9102e-02, -1.4199e-01,  3.4467e-02, -2.3243e-02,\n",
      "         -9.5813e-02,  1.1711e-01,  2.7243e-02, -1.4736e-01,  7.2981e-02,\n",
      "          1.5606e-01,  8.6626e-02,  2.4720e-02, -3.5073e-02,  4.5822e-02,\n",
      "          2.3167e-01, -5.4848e-02, -1.9567e-02, -3.1857e-02, -7.9079e-02,\n",
      "          1.8935e-01, -3.4025e-02,  6.7199e-02, -4.9505e-02,  1.0925e-02,\n",
      "          8.5610e-02,  1.7720e-01,  1.6818e-01,  1.0490e-01,  1.1439e-01,\n",
      "         -7.2743e-02,  1.9945e-01, -7.8758e-02,  4.8855e-02,  5.1238e-02,\n",
      "         -1.3110e-01, -1.8551e-02, -6.7861e-02, -1.6657e-01, -1.7531e-01,\n",
      "          3.4233e-01, -9.1541e-03, -8.4216e-02,  1.4466e-01,  5.8276e-03,\n",
      "         -7.0845e-02, -1.2919e-02, -3.9269e-02,  2.9193e-02,  1.6822e-02,\n",
      "         -7.4784e-02, -3.4073e-02,  2.6759e-01, -1.8663e-02, -1.2595e-01,\n",
      "         -1.6392e-01,  9.9778e-02,  8.3720e-04, -1.3183e-01,  7.1139e-02,\n",
      "          2.4727e-03,  2.0463e-01, -6.1339e-02, -5.4622e-02, -1.0156e-02,\n",
      "          1.0810e-01,  4.2835e-02,  2.3193e-02,  5.0068e-02, -7.6707e-02,\n",
      "          2.0952e-01,  1.8459e-01,  9.2535e-02, -1.7822e-01, -3.9080e-02,\n",
      "          7.1010e-02,  1.5429e-01, -2.3957e-02, -1.2544e-01,  1.3323e-01,\n",
      "         -1.3666e-01,  7.0513e-02,  3.4952e-02, -1.9707e-01, -2.6031e-02,\n",
      "         -1.9003e-01,  1.0647e-01, -6.4873e-02, -3.1036e-01,  2.5055e-02,\n",
      "          2.6466e-02,  1.8564e-02,  3.5974e-04,  1.3604e-01, -9.5981e-02,\n",
      "         -1.0782e-01,  5.2476e-02,  1.4423e-01, -1.7238e-01, -1.1136e-01,\n",
      "          1.3674e-01,  1.3337e-01, -3.6918e-02,  2.2534e-01,  1.5074e-01,\n",
      "          2.9448e-02,  4.0833e-02, -7.9977e-02,  7.6825e-02, -1.6149e-01,\n",
      "          1.8547e-03,  2.3219e-02, -4.6354e-03,  5.0308e-02, -4.5174e-02,\n",
      "         -6.7638e-03, -7.6198e-02,  3.3699e-02,  5.5131e-02, -2.2361e-01,\n",
      "         -9.2616e-02, -1.9398e-02, -6.5985e-02, -6.9078e-02,  3.4133e-02,\n",
      "         -8.9098e-02, -1.1989e-01, -1.6961e-01, -1.8326e-01, -1.0272e-01,\n",
      "          1.3438e-01, -2.1737e-01,  1.6502e-01,  1.8320e-01, -5.1689e-02,\n",
      "          2.2183e-01,  1.0560e-01,  2.3522e-02,  3.0095e-02, -8.5450e-02,\n",
      "         -1.3916e-01,  7.3009e-02,  7.3593e-02, -7.2348e-02,  2.0794e-01,\n",
      "         -1.0055e-01,  2.7168e-02, -3.2766e-02, -7.3998e-02,  1.4751e-04,\n",
      "          5.9433e-03, -8.1036e-02, -7.1722e-02,  7.0696e-02,  6.1410e-03,\n",
      "          3.8073e-02,  1.7661e-01, -2.6770e-02, -2.2045e-01, -6.9164e-02,\n",
      "          8.4054e-02, -6.2806e-02,  1.0688e-01, -1.6934e-01, -4.9434e-02,\n",
      "          1.3195e-01,  1.0617e-01, -3.1666e-02, -1.0888e-01, -7.7772e-02,\n",
      "         -5.4747e-03,  4.5735e-02,  1.9427e-02, -8.9625e-02,  2.5645e-01,\n",
      "          1.2863e-01, -1.5935e-01, -8.9559e-02, -2.0752e-01,  5.8842e-02,\n",
      "          1.2826e-02,  5.5238e-02,  3.2599e-02, -1.1983e-01, -2.5502e-01,\n",
      "          4.1382e-02,  7.2031e-02,  1.4464e-03,  1.0382e-01, -2.6820e-02,\n",
      "         -2.2116e-02, -3.4539e-02,  6.4049e-02, -4.9049e-02, -2.0319e-02,\n",
      "          1.2961e-01, -1.1845e-01, -1.0021e-03,  2.4513e-02, -4.5494e-02,\n",
      "          1.5026e-01, -2.6351e-01, -5.4540e-02, -5.0724e-02, -1.7266e-01,\n",
      "         -1.6624e-01,  1.1723e-01,  1.2130e-02,  5.0562e-02, -8.7487e-03,\n",
      "         -6.3272e-02,  1.0220e-01,  2.5463e-01,  1.4743e-01, -4.6884e-02,\n",
      "         -7.6756e-02,  6.7515e-02, -6.5504e-03, -2.7806e-01, -2.5303e-02,\n",
      "         -1.3486e-01,  9.6214e-02, -9.4295e-02, -8.3864e-02, -4.0287e-02,\n",
      "         -5.2744e-03]], grad_fn=<AddmmBackward0>))\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_path = os.path.abspath('data/tattoos/input1-1.jpg')\n",
    "\n",
    "pil_img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# pil_img = pil_img.resize((32, 288), Image.LANCZOS)\n",
    "\n",
    "np_image = np.array(pil_img)\n",
    "\n",
    "np_image = np_image/255\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "np_image = (np_image - mean)/std\n",
    "\n",
    "np_image = np_image.transpose((2, 0, 1))\n",
    "\n",
    "tensor = torch.from_numpy(np_image).to(device, dtype=torch.float)\n",
    "print(tensor.shape)\n",
    "\n",
    "output = model.forward(tensor.unsqueeze_(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 1 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mToPILImage()\n\u001b[0;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m transform(output[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m300\u001b[39m))\n\u001b[1;32m      7\u001b[0m img\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/computer-vision-verification-experiments/lib/python3.11/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_pil_image(pic, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/computer-vision-verification-experiments/lib/python3.11/site-packages/torchvision/transforms/functional.py:273\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    271\u001b[0m     pic \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(pic, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should not have > 4 channels. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m channels.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 1 dimensions."
     ]
    }
   ],
   "source": [
    "transform = T.ToPILImage()\n",
    "\n",
    "img = transform(output[1][0][0])\n",
    "\n",
    "img = img.resize((300, 300))\n",
    "\n",
    "img.show()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer-vision-verification-experiments-kernel",
   "language": "python",
   "name": "computer-vision-verification-experiments-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
